# Diffusion probablistic model for Singing Voice Conversion(Diff-SVC)
## Singing Voice Conversion

---

## Diff-SVC
- 노래하는 목소리(가창)의 콘텐츠(가사·멜로디)와 톤(화자 특성)을 분리한 뒤, 콘텐츠는 유지하고 톤(목소리 화자성)만 바꾸어 출력하는 방식
- 핵심 설계는 디퓨전 모델(diffusion probabilistic model) 을 변환 모델(conversion module)으로 사용

### 1. 콘텐츠 특징 추출(Content encoder)
- 모델은 원본 노래 음성(소스 가창)에서 발음 내용/언어 정보(content) 를 추출
- [DiffSVC 논문](https://www1.se.cuhk.edu.hk/~hccl/publications/pub/1%20ASRU2021_DiffSVC_final_hccl.pdf?utm_source=chatgpt.com)에서는 Phonetic Posteriorgrams(PPG)을 사용해 발음 특징을 추출
- 동시에, 멜로디(피치)나 음량 등의 음악적 제어 인자(auxiliary features) 도 함께 추출(F0, loudness.)
- 이 블록의 목적은 목소리 화자 특성(톤)과 독립적인 내용 단위를 확보해 두는 것("무슨 말을 하고 노래하느냐")

### 2. 보조 특징 및 화자 특성 추출: 피치(F0), 음량(loudness) 등
- F0(기본 주파수) 및 loudness(음량) 등의 음악적 표현 요소가 변환 모델 입력에 포함(노래에서 멜로디/발성 억양 구현을 보조)
- 타깃 화자의 화자 임베딩(speaker embedding) 혹은 화자 특성을 나타내는 벡터도 입력으로 들어가서 화자톤을 반영
- 이렇게 해서 변환 모델은 "내용 + 멜로디·음량 + 목표화자톤" 정보를 모두 참고하여 출력 음성을 생성

### 3. 디퓨전 변환 모델(Denoising/Reverse process)
#### 디퓨전 모델 
- 노이즈 추가(forward process)를 통해 데이터 분포를 Gaussian noise 쪽으로 변화시키고, 그 후 reverse process에서 노이즈로부터 데이터를 점차 복원해 내는 생성모델

- DiffSVC에서는 멜 스펙트로그램(mel-spectrogram) 을 변환 대상 acoustic feature로 지정
- 노이즈가 더해진 스펙트로그램으로부터 원본 스펙트로그램을 복원하는 네트워크를 학습
- 입력 조건으로는 PPG(내용), F0 및 loudness(멜로디·음량 정보), 화자 임베딩 등이 포함
- 단순 스펙트럼을 변경 → 내용과 멜로디를 유지하면서 화자톤만 바꾸는 고품질 변환 구현

### 4. 보코더(Vocoder)나 스펙트로그램 → 파형 변환 모듈
- 디퓨전 모델이 생성한 멜 스펙트로그램을 실제 음성 파형(wav)으로 바꾸기 위해 보코더(vocoder) 혹은 직접 스펙트로그램→파형 합성 모듈이 필요(논문에서는 이 부분이 명시적으로 상세하지 않을 수 있지만, 실용 구현 시 필수)
- 출력 품질을 위해서는 보코더 품질, 샘플레이트, hop-size 등 설정이 중요

---

## Diff-SVC의 설계 특징 및 장단점
### 특징
1. 비병렬 학습 가능: 소스 가수와 타깃 가수가 동일 곡을 부른 병렬 데이터가 없어도(혹은 적어도) 콘텐츠/화자 분리를 통해 변환 가능하다는 장점이 있습니다. 

2. 디퓨전 모델 사용: 최근 이미지/오디오 생성에서 뛰어난 결과를 보인 디퓨전 모델을 SVC에 도입한 것이 큰 설계 포인트입니다. 노이즈 복원 방식이 자연스러운 음성에 유리합니다. 

3. 멜로디·음량 명시적 활용: 노래 특성상 피치·음량 변화가 많기 때문에, 이를 보조 인자로 사용하는 것이 음질 및 자연스러움에 기여합니다.

### 제약/단점
1. 연산·학습 비용 증가: 디퓨전 모델은 반복 복원(iterative) 구조이기 때문에 단순 GAN보다 학습 및 추론 비용이 높을 수 있습니다.

2. 데이터 품질 요구: 노래 음성은 발음·비브라토·음량 변화 등이 일반 대화 음성보다 복잡하므로, 품질 좋은 데이터가 중요합니다.

3. 실시간 적용에 난이도 존재: 디퓨전 방식 특성상 반복 스텝이 많으면 실시간 적용이 어렵습니다.

4. 보코더 품질 의존: 스펙트로그램까지는 잘 생성되더라도 보코더가 낮으면 최종 음질이 저하될 수 있습니다.

---

## 로직 구성
| 구분                                      | 주요 모듈                            | 입력           | 출력                  | 역할 요약                      |
| --------------------------------------- | -------------------------------- | ------------ | ------------------- | -------------------------- |
| **1️⃣ 인코더 (Encoder)**                   | 콘텐츠 인코더, F0·loudness 인코더, 화자 인코더 | 원본 노래(소스 보컬) | 노래의 언어·리듬·멜로디·화자 특성 | **노래의 특성 추출**              |
| **2️⃣ 디코더 (Decoder / Diffusion Model)** | 조건 결합 모듈 + 확산 디코더                | 인코더가 만든 특징들  | 타깃 화자의 멜 스펙트로그램     | **타깃 음성 특성에 맞게 스펙트로그램 변환** |
| **3️⃣ 보코더 (Vocoder)**                   | HiFi-GAN 등                       | 수정된 멜 스펙트로그램 | 최종 음성 파형(WAV)       | **스펙트로그램을 실제 음성으로 변환**     |

### 각 단계의 작동 요약
#### 1. 인코더 — 노래의 특성 추출
- 콘텐츠 인코더: 음성에서 발음·가사·리듬 패턴을 뽑아 화자 정보와 분리합니다. (예: PPG, HuBERT, ContentVec 등)
- F0 인코더: 노래의 피치(멜로디) 곡선을 정밀하게 추출합니다.
- 스피커 인코더: 타깃 화자의 음색 특성을 벡터(embedding)로 표현합니다.

→ *무엇을 부르는가(콘텐츠)*와 *어떤 높이로 부르는가(피치)*가 분리되어, 이후 변환 단계의 입력 조건으로 전달됩니다.

#### 2. 디코더 — 타깃 음성 특성에 맞게 스펙트로그램 수정
- 입력 조건: 콘텐츠 + F0 + loudness + 타깃 화자 임베딩
- 작동: 확산모델(diffusion model)의 역확산(denoising) 과정을 통해 원본 노래의 내용·멜로디는 유지하면서 타깃 화자의 음색으로 변환된 멜 스펙트로그램을 생성합니다.

→ 이 단계는 *보이스 스타일 트랜스퍼* 의 핵심입니다.

#### 3. 보코더 — 스펙트로그램을 실제 음성으로 합성
- 디코더가 만든 멜 스펙트로그램을 입력받아 실제 오디오 신호(PCM 파형)로 복원합니다.
- 보통 HiFi-GAN·WaveRNN 같은 범용 보코더를 사용합니다.

→ 결과적으로, 완성된 WAV 파일이나 스트림 형태의 타깃 보컬이 출력됩니다.
